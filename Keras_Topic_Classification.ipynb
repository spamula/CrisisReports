{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Topic Classification Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function takes around 30.321 seconds to run\n"
     ]
    }
   ],
   "source": [
    "# DEFINE DIRECTORY PATH\n",
    "path_to_json = 'capstone/TrainingData2014/'\n",
    "\n",
    "# CREATE LIST OF FILES FROM THE DIRECTORY\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "# DEFINE PANDAS DATAFRAME\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# LOOP THROUGH FILES, READ IN JSON AND BUILD DATAFRAME\n",
    "for index, js in enumerate(json_files):\n",
    "    json_data = pd.read_json(os.path.join(path_to_json, js), 'r')\n",
    "    df = df.append(json_data)\n",
    "\n",
    "# LOOK AT TOPIC DICTIONARY AND GET A TOPIC COUNT\n",
    "topic_file = open('capstone/topicDictionary.txt', 'r')\n",
    "topics = topic_file.read().split('\\r\\n')\n",
    "\n",
    "# SPLITTING THE ELEMENTS OF THE JSON INTO TEXT, PUBLICATION DATE AND TOPICS\n",
    "df['text'] = df.TrainingData.apply(lambda x: x['bodyText'])\n",
    "df['pubdate'] = df.TrainingData.apply(lambda x: x['webPublicationDate'])\n",
    "df['topics'] = df.TrainingData.apply(lambda x: x['topics'])\n",
    "\n",
    "# DROP FIRST TWO COLUMNS\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.drop('TrainingData', axis=1, inplace=True)\n",
    "\n",
    "# DEFINE FUNCTION TO CREATE OUR DATAFRAME\n",
    "def topic_col(x):\n",
    "    a = 0\n",
    "    for elem in x:\n",
    "        if elem == topic:\n",
    "            a = 1\n",
    "    return a\n",
    "\n",
    "# RUN TOPIC COL FUNCTION ON ALL DATA\n",
    "time1 = time.time()\n",
    "for topic in topics:\n",
    "    df[topic] = df['topics'].map(topic_col)\n",
    "time2 = time.time()\n",
    "time_in_s = (time2-time1)\n",
    "print 'Function takes around %0.3f seconds to run' % (time_in_s)\n",
    "\n",
    "X = df['text']\n",
    "y = df['afghanistan']\n",
    "\n",
    "# CREATE A TRAIN AND TEST SPLIT FOR THE DATA WITH A TEST SIZE OF 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the topic of Afghanistan with approximately 400 topic labelled articles out of 111,200 articles, we will see how well our model can predict turning the text of the articles into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler(topic, df):\n",
    "    \n",
    "    # SELECT INDICES OF TOPIC ARTICLES\n",
    "    topicindexes = df[df[topic] == 1].index.tolist()\n",
    "    \n",
    "    # FIND COUNT OF TOPIC ARTICLES\n",
    "    articlecount = len(topicindexes)\n",
    "    \n",
    "    if articlecount < 10:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(articlecount*27, random_state=42).index.tolist()\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes \n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "        X = X.append(df.iloc[topicindexes]['text'])\n",
    "        y = y.append(df.iloc[topicindexes][topic])\n",
    "        X = X.append(df.iloc[topicindexes]['text'])\n",
    "        y = y.append(df.iloc[topicindexes][topic])\n",
    "        X = X.append(df.iloc[topicindexes]['text']).reset_index(drop=True)\n",
    "        y = y.append(df.iloc[topicindexes][topic]).reset_index(drop=True)\n",
    "        \n",
    "    elif articlecount < 100:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(articlecount*9, random_state=42).index.tolist()\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes \n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "        X = X.append(df.iloc[topicindexes]['text']).reset_index(drop=True)\n",
    "        y = y.append(df.iloc[topicindexes][topic]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        topicindexes = df[df[topic] == 1].sample(100).index.tolist()\n",
    "        \n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(len(topicindexes)*9, random_state=42).index.tolist()\n",
    "        nonarticlecount = len(nontopicarticlesindexes)\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes\n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    110799\n",
       "1       399\n",
       "Name: afghanistan, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['afghanistan'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET X AND Y\n",
    "X, y = sampler('afghanistan', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Taliban have released a video of the momen...\n",
       "1    Two years ago, I came to New Hampshire to redi...\n",
       "2    When Darwish looked out of his new living room...\n",
       "3    The bus that rumbles through the Kandahar afte...\n",
       "4    After an exhausting and contentious election p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK MAIN TEXT\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    900\n",
       "1    100\n",
       "Name: afghanistan, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK TARGET VALUE COUNTS\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: afghanistan, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK TARGET\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A TRAIN AND TEST SPLIT FOR THE DATA WITH A TEST SIZE OF 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 'train sequences')\n",
      "(200, 'test sequences')\n",
      "(2, 'classes')\n",
      "Vectorizing sequence data...\n",
      "('x_train shape:', (800,))\n",
      "('x_test shape:', (200,))\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "('y_train shape:', (800, 2))\n",
      "('y_test shape:', (200, 2))\n",
      "Building model...\n",
      "Train on 720 samples, validate on 80 samples\n",
      "Epoch 1/5\n",
      "720/720 [==============================] - 3s - loss: 0.6846 - acc: 0.9000 - val_loss: 0.6751 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "720/720 [==============================] - 2s - loss: 0.6670 - acc: 0.9000 - val_loss: 0.6578 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "720/720 [==============================] - 2s - loss: 0.6502 - acc: 0.9000 - val_loss: 0.6413 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "720/720 [==============================] - 2s - loss: 0.6341 - acc: 0.9000 - val_loss: 0.6255 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "720/720 [==============================] - 2s - loss: 0.6187 - acc: 0.9000 - val_loss: 0.6106 - val_acc: 0.9000\n",
      "192/200 [===========================>..] - ETA: 0s('Test score:', 0.6105796241760254)\n",
      "('Test accuracy:', 0.90000000000000002)\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
