{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Vocabulary Function with Spacy whilst modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If sample size is less than 10, create more features\n",
    "Adjust number of features depending on the size of our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary using Spacy Tokenizer in our\n",
    "Fit using the same tokenizer for gridsearched model to cut down on processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activism\n",
      "Function takes around 0.336 seconds to run\n",
      "afghanistan\n",
      "Function takes around 0.268 seconds to run\n",
      "aid\n",
      "Function takes around 0.229 seconds to run\n",
      "algerianhostagecrisis\n",
      "Function takes around 0.294 seconds to run\n",
      "alqaida\n",
      "Function takes around 0.235 seconds to run\n",
      "alshabaab\n",
      "Function takes around 0.318 seconds to run\n",
      "antiwar\n",
      "Function takes around 0.310 seconds to run\n",
      "arabandmiddleeastprotests\n",
      "Function takes around 0.268 seconds to run\n",
      "armstrade\n",
      "Function takes around 0.209 seconds to run\n",
      "australianguncontrol\n",
      "Function takes around 0.241 seconds to run\n",
      "australiansecurityandcounterterrorism\n",
      "Function takes around 0.344 seconds to run\n",
      "bastilledaytruckattack\n",
      "Function takes around 0.262 seconds to run\n",
      "belgium\n",
      "Function takes around 0.250 seconds to run\n",
      "berlinchristmasmarketattack\n",
      "Function takes around 0.298 seconds to run\n",
      "bigdata\n",
      "Function takes around 0.251 seconds to run\n",
      "biometrics\n",
      "Function takes around 0.251 seconds to run\n",
      "bokoharam\n",
      "Function takes around 0.279 seconds to run\n",
      "bostonmarathonbombing\n",
      "Function takes around 0.299 seconds to run\n",
      "britisharmy\n",
      "Function takes around 0.273 seconds to run\n",
      "brusselsattacks\n",
      "Function takes around 0.339 seconds to run\n",
      "cameroon\n",
      "Function takes around 0.269 seconds to run\n",
      "carers\n",
      "Function takes around 0.269 seconds to run\n",
      "charliehebdoattack\n",
      "Function takes around 0.257 seconds to run\n",
      "chemicalweapons\n",
      "Function takes around 0.287 seconds to run\n",
      "clusterbombs\n",
      "Function takes around 0.271 seconds to run\n",
      "cobra\n",
      "Function takes around 0.568 seconds to run\n",
      "conflictanddevelopment\n",
      "Function takes around 0.626 seconds to run\n",
      "controversy\n",
      "Function takes around 0.446 seconds to run\n",
      "criminaljustice\n",
      "Function takes around 0.529 seconds to run\n",
      "cybercrime\n",
      "Function takes around 0.581 seconds to run\n",
      "cyberwar\n",
      "Function takes around 0.685 seconds to run\n",
      "darknet\n",
      "Function takes around 0.472 seconds to run\n",
      "dataprotection\n",
      "Function takes around 0.808 seconds to run\n",
      "debate\n",
      "Function takes around 0.539 seconds to run\n",
      "defence\n",
      "Function takes around 0.411 seconds to run\n",
      "deflation\n",
      "Function takes around 0.401 seconds to run\n",
      "drones\n",
      "Function takes around 0.348 seconds to run\n",
      "drugs\n",
      "Function takes around 0.377 seconds to run\n",
      "drugspolicy\n",
      "Function takes around 0.238 seconds to run\n",
      "drugstrade\n",
      "Function takes around 0.261 seconds to run\n",
      "earthquakes\n",
      "Function takes around 0.216 seconds to run\n",
      "ebola\n",
      "Function takes around 0.236 seconds to run\n",
      "economy\n",
      "Function takes around 0.223 seconds to run\n",
      "egypt\n",
      "Function takes around 0.222 seconds to run\n",
      "encryption\n",
      "Function takes around 0.407 seconds to run\n",
      "energy\n",
      "Function takes around 0.353 seconds to run\n",
      "espionage\n",
      "Function takes around 0.311 seconds to run\n",
      "ethics\n",
      "Function takes around 0.240 seconds to run\n",
      "europeanarrestwarrant\n",
      "Function takes around 0.313 seconds to run\n",
      "europeancourtofhumanrights\n",
      "Function takes around 0.353 seconds to run\n",
      "events\n",
      "Function takes around 0.271 seconds to run\n",
      "extradition\n",
      "Function takes around 0.317 seconds to run\n",
      "famine\n",
      "Function takes around 0.267 seconds to run\n",
      "farright\n",
      "Function takes around 0.274 seconds to run\n",
      "firefighters\n",
      "Function takes around 0.288 seconds to run\n",
      "forensicscience\n",
      "Function takes around 0.421 seconds to run\n",
      "france\n",
      "Function takes around 0.186 seconds to run\n",
      "francetrainattack\n",
      "Function takes around 0.207 seconds to run\n",
      "freedomofspeech\n",
      "Function takes around 0.174 seconds to run\n",
      "genevaconventions\n",
      "Function takes around 0.179 seconds to run\n",
      "germany\n",
      "Function takes around 0.147 seconds to run\n",
      "guncrime\n",
      "Function takes around 0.114 seconds to run\n",
      "hacking\n",
      "Function takes around 0.142 seconds to run\n",
      "hashtags\n",
      "Function takes around 0.108 seconds to run\n",
      "helicoptercrashes\n",
      "Function takes around 0.131 seconds to run\n",
      "humanitarianresponse\n",
      "Function takes around 0.123 seconds to run\n",
      "humanrights\n",
      "Function takes around 0.138 seconds to run\n",
      "humanrightsact\n",
      "Function takes around 0.190 seconds to run\n",
      "humantrafficking\n",
      "Function takes around 0.148 seconds to run\n",
      "immigration\n",
      "Function takes around 0.125 seconds to run\n",
      "india\n",
      "Function takes around 0.101 seconds to run\n",
      "indonesia\n",
      "Function takes around 0.100 seconds to run\n",
      "internallydisplacedpeople\n",
      "Function takes around 0.135 seconds to run\n",
      "internationalcourtofjustice\n",
      "Function takes around 0.114 seconds to run\n",
      "internationalcriminaljustice\n",
      "Function takes around 0.146 seconds to run\n",
      "internetsafety\n",
      "Function takes around 0.100 seconds to run\n",
      "iraq\n",
      "Function takes around 0.116 seconds to run\n",
      "isis\n",
      "Function takes around 0.094 seconds to run\n",
      "israel\n",
      "Function takes around 0.115 seconds to run\n",
      "jordan\n",
      "Function takes around 0.105 seconds to run\n",
      "jubilee\n",
      "Function takes around 0.107 seconds to run\n",
      "judiciary\n",
      "Function takes around 0.119 seconds to run\n",
      "july7\n",
      "Function takes around 0.137 seconds to run\n",
      "justiceandsecurity\n",
      "Function takes around 0.125 seconds to run\n",
      "kenya\n",
      "Function takes around 0.094 seconds to run\n",
      "knifecrime\n",
      "Function takes around 0.130 seconds to run\n",
      "lebanon\n",
      "Function takes around 0.099 seconds to run\n",
      "libya\n",
      "Function takes around 0.121 seconds to run\n",
      "localgovernment\n",
      "Function takes around 0.106 seconds to run\n",
      "logistics\n",
      "Function takes around 0.119 seconds to run\n",
      "london\n",
      "Function takes around 0.098 seconds to run\n",
      "londonriots\n",
      "Function takes around 0.106 seconds to run\n",
      "malaysia\n",
      "Function takes around 0.129 seconds to run\n",
      "mali\n",
      "Function takes around 0.095 seconds to run\n",
      "malware\n",
      "Function takes around 0.124 seconds to run\n",
      "metropolitanpolice\n",
      "Function takes around 0.132 seconds to run\n",
      "middleeastpeacetalks\n",
      "Function takes around 0.206 seconds to run\n",
      "migration\n",
      "Function takes around 0.174 seconds to run\n",
      "military\n",
      "Function takes around 0.237 seconds to run\n",
      "ministryofdefence\n",
      "Function takes around 0.326 seconds to run\n",
      "morocco\n",
      "Function takes around 0.123 seconds to run\n",
      "mrsa\n",
      "Function takes around 0.104 seconds to run\n",
      "mumbaiterrorattacks\n",
      "Function takes around 0.130 seconds to run\n",
      "munichshooting\n",
      "Function takes around 0.125 seconds to run\n",
      "naturaldisasters\n",
      "Function takes around 0.105 seconds to run\n",
      "nigeria\n",
      "Function takes around 0.109 seconds to run\n",
      "nuclearweapons\n",
      "Function takes around 0.113 seconds to run\n",
      "occupy\n",
      "Function takes around 0.113 seconds to run\n",
      "organisedcrime\n",
      "Function takes around 0.122 seconds to run\n",
      "orlandoterrorattack\n",
      "Function takes around 0.139 seconds to run\n",
      "osamabinladen\n",
      "Function takes around 0.100 seconds to run\n",
      "paris\n",
      "Function takes around 0.113 seconds to run\n",
      "parisattacks\n",
      "Function takes around 0.115 seconds to run\n",
      "peaceandreconciliation\n",
      "Function takes around 0.155 seconds to run\n",
      "philippines\n",
      "Function takes around 0.109 seconds to run\n",
      "piracy\n",
      "Function takes around 0.136 seconds to run\n",
      "planecrashes\n",
      "Function takes around 0.107 seconds to run\n",
      "police\n",
      "Function takes around 0.142 seconds to run\n",
      "protest\n",
      "Function takes around 0.092 seconds to run\n",
      "refugees\n",
      "Function takes around 0.110 seconds to run\n",
      "religion\n",
      "Function takes around 0.102 seconds to run\n",
      "retirementage\n",
      "Function takes around 0.107 seconds to run\n",
      "rio20earthsummit\n",
      "Function takes around 0.134 seconds to run\n",
      "royalairforce\n",
      "Function takes around 0.101 seconds to run\n",
      "royalnavy\n",
      "Function takes around 0.115 seconds to run\n",
      "russia\n",
      "Function takes around 0.096 seconds to run\n",
      "sanbernardinoshooting\n",
      "Function takes around 0.145 seconds to run\n",
      "saudiarabia\n",
      "Function takes around 0.112 seconds to run\n",
      "september11\n",
      "Function takes around 0.115 seconds to run\n",
      "slavery\n",
      "Function takes around 0.101 seconds to run\n",
      "somalia\n",
      "Function takes around 0.116 seconds to run\n",
      "southafrica\n",
      "Function takes around 0.111 seconds to run\n",
      "southchinasea\n",
      "Function takes around 0.108 seconds to run\n",
      "stopandsearch\n",
      "Function takes around 0.127 seconds to run\n",
      "surveillance\n",
      "Function takes around 0.112 seconds to run\n",
      "sydneysiege\n",
      "Function takes around 0.115 seconds to run\n",
      "syria\n",
      "Function takes around 0.087 seconds to run\n",
      "taliban\n",
      "Function takes around 0.104 seconds to run\n",
      "terrorism\n",
      "Function takes around 0.102 seconds to run\n",
      "thailand\n",
      "Function takes around 0.093 seconds to run\n",
      "torture\n",
      "Function takes around 0.107 seconds to run\n",
      "traincrashes\n",
      "Function takes around 0.110 seconds to run\n",
      "transport\n",
      "Function takes around 0.122 seconds to run\n",
      "tunisiaattack2015\n",
      "Function takes around 0.102 seconds to run\n",
      "turkey\n",
      "Function takes around 0.090 seconds to run\n",
      "turkeycoupattempt\n",
      "Function takes around 0.117 seconds to run\n",
      "ukcrime\n",
      "Function takes around 0.090 seconds to run\n",
      "uksecurity\n",
      "Function takes around 0.119 seconds to run\n",
      "uksupremecourt\n",
      "Function takes around 0.110 seconds to run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercoverpoliceandpolicing\n",
      "Function takes around 0.129 seconds to run\n",
      "unitednations\n",
      "Function takes around 0.173 seconds to run\n",
      "usguncontrol\n",
      "Function takes around 0.278 seconds to run\n",
      "values\n",
      "Function takes around 0.110 seconds to run\n",
      "warcrimes\n",
      "Function takes around 0.118 seconds to run\n",
      "warreporting\n",
      "Function takes around 0.127 seconds to run\n",
      "weaponstechnology\n",
      "Function takes around 0.122 seconds to run\n",
      "womeninbusiness\n",
      "Function takes around 0.138 seconds to run\n",
      "woolwichattack\n",
      "Function takes around 0.101 seconds to run\n",
      "worldmigration\n",
      "Function takes around 0.120 seconds to run\n",
      "zikavirus\n",
      "Function takes around 0.102 seconds to run\n"
     ]
    }
   ],
   "source": [
    "# DEFINE DIRECTORY PATH\n",
    "path_to_json = './TrainingData2014/'\n",
    "\n",
    "# CREATE LIST OF FILES FROM THE DIRECTORY\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "# DEFINE PANDAS DATAFRAME\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# LOOP THROUGH FILES, READ IN JSON AND BUILD DATAFRAME\n",
    "for index, js in enumerate(json_files):\n",
    "    json_data = pd.read_json(os.path.join(path_to_json, js), 'r')\n",
    "    df = df.append(json_data)\n",
    "    \n",
    "# LOOK AT TOPIC DICTIONARY AND GET A TOPIC COUNT\n",
    "topic_file = open('./topicDictionary.txt', 'r')\n",
    "topics = topic_file.read().split('\\r\\n')\n",
    "\n",
    "# SPLITTING THE ELEMENTS OF THE JSON INTO TEXT, PUBLICATION DATE AND TOPICS\n",
    "df['text'] = df.TrainingData.apply(lambda x: x['bodyText'])\n",
    "df['pubdate'] = df.TrainingData.apply(lambda x: x['webPublicationDate'])\n",
    "df['topics'] = df.TrainingData.apply(lambda x: x['topics'])\n",
    "\n",
    "# DROP FIRST TWO COLUMNS\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.drop('TrainingData', axis=1, inplace=True)\n",
    "\n",
    "# DEFINE FUNCTION TO CREATE OUR DATAFRAME\n",
    "def topic_col(x):\n",
    "    a = 0\n",
    "    for elem in x:\n",
    "        if elem == topic:\n",
    "            a = 1\n",
    "    return a\n",
    "\n",
    "# RUN TOPIC COL FUNCTION ON ALL DATA\n",
    "for topic in topics:\n",
    "    time1 = time.time()\n",
    "    df[topic] = df['topics'].map(topic_col)\n",
    "    print topic\n",
    "    time2 = time.time()\n",
    "    time_in_s = (time2-time1)\n",
    "    print 'Function takes around %0.3f seconds to run' % (time_in_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE FINAL TEST DF\n",
    "testdf = pd.read_json('/Users/sudheerpamula/Downloads/TestData.json', 'r')\n",
    "testdf['text'] = testdf.TestData.apply(lambda x: x['bodyText'])\n",
    "testdf['pubdate'] = testdf.TestData.apply(lambda x: x['webPublicationDate'])\n",
    "testdf['topics'] = testdf.TestData.apply(lambda x: x['topics'])\n",
    "testdf.drop('TestData', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE SUBMISSION CSV DATAFRAME\n",
    "submission = pd.DataFrame(index=testdf.index, columns=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler(topic, df):\n",
    "    \n",
    "    # SELECT INDICES OF TOPIC ARTICLES\n",
    "    topicindexes = df[df[topic] == 1].index.tolist()\n",
    "    \n",
    "    # FIND COUNT OF TOPIC ARTICLES\n",
    "    articlecount = len(topicindexes)\n",
    "    \n",
    "    if articlecount < 10:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(articlecount*27, random_state=42).index.tolist()\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes \n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "        X = X.append(df.iloc[topicindexes]['text'])\n",
    "        y = y.append(df.iloc[topicindexes][topic])\n",
    "        X = X.append(df.iloc[topicindexes]['text'])\n",
    "        y = y.append(df.iloc[topicindexes][topic])\n",
    "        X = X.append(df.iloc[topicindexes]['text']).reset_index(drop=True)\n",
    "        y = y.append(df.iloc[topicindexes][topic]).reset_index(drop=True)\n",
    "        \n",
    "    elif articlecount < 100:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(articlecount*9, random_state=42).index.tolist()\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes \n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "        X = X.append(df.iloc[topicindexes]['text']).reset_index(drop=True)\n",
    "        y = y.append(df.iloc[topicindexes][topic]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        # SELECT NON TOPIC INDICES\n",
    "        topicindexes = df[df[topic] == 1].sample(100).index.tolist()\n",
    "        \n",
    "        nontopicarticlesindexes = df[df[topic] == 0].sample(len(topicindexes)*9, random_state=42).index.tolist()\n",
    "        nonarticlecount = len(nontopicarticlesindexes)\n",
    "        # CREATE LIST OF COMBINED INDICES\n",
    "        sampleindex = topicindexes + nontopicarticlesindexes\n",
    "        # CREATE NEW DATAFRAME\n",
    "        X = df.iloc[sampleindex]['text'].reset_index(drop=True)\n",
    "        y = df.iloc[sampleindex][topic].reset_index(drop=True)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADDED 4 THAT WERE NOT IN 2014 ARTICLES\n",
    "topics_0 = ['activism', 'turkeycoupattempt', 'tunisiaattack2015', 'sanbernardinoshooting', 'peaceandreconciliation',\n",
    "            'parisattacks', 'orlandoterrorattack', 'munichshooting', 'francetrainattack', 'charliehebdoattack',\n",
    "            'brusselsattacks', 'berlinchristmasmarketattack','bastilledaytruckattack','zikavirus', 'jubilee', 'values', 'rio20earthsummit', 'algerianhostagecrisis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "# Every step in a pipeline needs to be a \"transformer\". Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Worst Features: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "        spacyworstvocab[topic] = feat\n",
    "    print(\"Strongest Features: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "        spacyvocab[topic] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vectorizer__max_df': (0.15, 0.25),\n",
    "    'tfidf__norm': ('l2', 'l1'),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'vectorizer__binary': (True, False),\n",
    "    'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacyvocab = {}\n",
    "spacyworstvocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:65: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 20.1min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-988849d857c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,2))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()),\n",
    "                 ('vectorizer', vectorizer), \n",
    "                 ('tfidf', TfidfTransformer()), \n",
    "                 ('ss', StandardScaler(with_mean=False)), \n",
    "                 ('clf', clf)])\n",
    "\n",
    "for topic in topics:\n",
    "    if topic in topics_0:\n",
    "        submission[topic] = 0\n",
    "    \n",
    "    else:\n",
    "        X, y = sampler(topic, df)\n",
    "        train, test, labelsTrain, labelsTest = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "        \n",
    "        pipe.fit(train, labelsTrain)\n",
    "\n",
    "        grid_search = GridSearchCV(pipe, parameters, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(train, labelsTrain)\n",
    "\n",
    "        preds = grid_search.best_estimator_.predict(test)\n",
    "        topicf1score = f1_score(labelsTest, preds)\n",
    "        print(\"----------------------------------------------------------------------------------------------\")\n",
    "        print topic, \"F1 score:\", topicf1score\n",
    "        print \"Grid Search Best Params:\", grid_search.best_estimator_.get_params\n",
    "        \n",
    "        # PICKLE GRID SEARCH BEST PARAMETERS\n",
    "        with open('Pickles/gs_logreg'+str(topic)+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(spacyvocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # MAKE FINAL PREDICTIONS\n",
    "        preds_final = grid_search.best_estimator_.predict(testdf['text'])\n",
    "        \n",
    "        # WRITE TO CSV\n",
    "        submission[topic] = preds_final\n",
    "        submission.to_csv('submission_spacytokenizer_2.csv')\n",
    "        print(\"----------------------------------------------------------------------------------------------\")\n",
    "        print(\"Predictions Completed on Test Data\")\n",
    "        \n",
    "        print(\"----------------------------------------------------------------------------------------------\")\n",
    "        print(\"Top 10 features used to predict: \")\n",
    "        printNMostInformative(vectorizer, clf, 30)\n",
    "        \n",
    "        print(\"----------------------------------------------------------------------------------------------\")\n",
    "        print(\"Pickled Vocabulary to File: \", topic)\n",
    "        with open('spacyvocab.pickle', 'wb') as handle:\n",
    "            pickle.dump(spacyvocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('spacyworstvocab.pickle', 'wb') as handle:\n",
    "            pickle.dump(spacyworstvocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activism',\n",
       " 'afghanistan',\n",
       " 'aid',\n",
       " 'algerianhostagecrisis',\n",
       " 'alqaida',\n",
       " 'alshabaab',\n",
       " 'antiwar',\n",
       " 'arabandmiddleeastprotests',\n",
       " 'armstrade',\n",
       " 'australianguncontrol',\n",
       " 'australiansecurityandcounterterrorism',\n",
       " 'bastilledaytruckattack',\n",
       " 'belgium',\n",
       " 'berlinchristmasmarketattack',\n",
       " 'bigdata',\n",
       " 'biometrics',\n",
       " 'bokoharam',\n",
       " 'bostonmarathonbombing',\n",
       " 'britisharmy',\n",
       " 'brusselsattacks',\n",
       " 'cameroon',\n",
       " 'carers',\n",
       " 'charliehebdoattack',\n",
       " 'chemicalweapons',\n",
       " 'clusterbombs',\n",
       " 'cobra',\n",
       " 'conflictanddevelopment',\n",
       " 'controversy',\n",
       " 'criminaljustice',\n",
       " 'cybercrime',\n",
       " 'cyberwar',\n",
       " 'darknet',\n",
       " 'dataprotection',\n",
       " 'debate',\n",
       " 'defence',\n",
       " 'deflation',\n",
       " 'drones',\n",
       " 'drugs',\n",
       " 'drugspolicy',\n",
       " 'drugstrade',\n",
       " 'earthquakes',\n",
       " 'ebola',\n",
       " 'economy',\n",
       " 'egypt',\n",
       " 'encryption',\n",
       " 'energy',\n",
       " 'espionage',\n",
       " 'ethics',\n",
       " 'europeanarrestwarrant',\n",
       " 'europeancourtofhumanrights',\n",
       " 'events',\n",
       " 'extradition',\n",
       " 'famine',\n",
       " 'farright',\n",
       " 'firefighters',\n",
       " 'forensicscience',\n",
       " 'france',\n",
       " 'francetrainattack',\n",
       " 'freedomofspeech',\n",
       " 'genevaconventions',\n",
       " 'germany',\n",
       " 'guncrime',\n",
       " 'hacking',\n",
       " 'hashtags',\n",
       " 'helicoptercrashes',\n",
       " 'humanitarianresponse',\n",
       " 'humanrights',\n",
       " 'humanrightsact',\n",
       " 'humantrafficking',\n",
       " 'immigration',\n",
       " 'india',\n",
       " 'indonesia',\n",
       " 'internallydisplacedpeople',\n",
       " 'internationalcourtofjustice',\n",
       " 'internationalcriminaljustice',\n",
       " 'internetsafety',\n",
       " 'iraq',\n",
       " 'isis',\n",
       " 'israel',\n",
       " 'jordan',\n",
       " 'jubilee',\n",
       " 'judiciary',\n",
       " 'july7',\n",
       " 'justiceandsecurity',\n",
       " 'kenya',\n",
       " 'knifecrime',\n",
       " 'lebanon',\n",
       " 'libya',\n",
       " 'localgovernment',\n",
       " 'logistics',\n",
       " 'london',\n",
       " 'londonriots',\n",
       " 'malaysia',\n",
       " 'mali',\n",
       " 'malware',\n",
       " 'metropolitanpolice',\n",
       " 'middleeastpeacetalks',\n",
       " 'migration',\n",
       " 'military',\n",
       " 'ministryofdefence',\n",
       " 'morocco',\n",
       " 'mrsa',\n",
       " 'mumbaiterrorattacks',\n",
       " 'munichshooting',\n",
       " 'naturaldisasters',\n",
       " 'nigeria',\n",
       " 'nuclearweapons',\n",
       " 'occupy',\n",
       " 'organisedcrime',\n",
       " 'orlandoterrorattack',\n",
       " 'osamabinladen',\n",
       " 'paris',\n",
       " 'parisattacks',\n",
       " 'peaceandreconciliation',\n",
       " 'philippines',\n",
       " 'piracy',\n",
       " 'planecrashes',\n",
       " 'police',\n",
       " 'protest',\n",
       " 'refugees',\n",
       " 'religion',\n",
       " 'retirementage',\n",
       " 'rio20earthsummit',\n",
       " 'royalairforce',\n",
       " 'royalnavy',\n",
       " 'russia',\n",
       " 'sanbernardinoshooting',\n",
       " 'saudiarabia',\n",
       " 'september11',\n",
       " 'slavery',\n",
       " 'somalia',\n",
       " 'southafrica',\n",
       " 'southchinasea',\n",
       " 'stopandsearch',\n",
       " 'surveillance',\n",
       " 'sydneysiege',\n",
       " 'syria',\n",
       " 'taliban',\n",
       " 'terrorism',\n",
       " 'thailand',\n",
       " 'torture',\n",
       " 'traincrashes',\n",
       " 'transport',\n",
       " 'tunisiaattack2015',\n",
       " 'turkey',\n",
       " 'turkeycoupattempt',\n",
       " 'ukcrime',\n",
       " 'uksecurity',\n",
       " 'uksupremecourt',\n",
       " 'undercoverpoliceandpolicing',\n",
       " 'unitednations',\n",
       " 'usguncontrol',\n",
       " 'values',\n",
       " 'warcrimes',\n",
       " 'warreporting',\n",
       " 'weaponstechnology',\n",
       " 'womeninbusiness',\n",
       " 'woolwichattack',\n",
       " 'worldmigration',\n",
       " 'zikavirus']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
